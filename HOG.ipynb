{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import shutil\n",
    "import seaborn as sn\n",
    "from os import listdir, walk, getcwd, mkdir\n",
    "from os.path import isfile, join, sep\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# import resizing snippets\n",
    "from conversion import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Structure\n",
    "# \n",
    "# +---.ipynb_checkpoints\n",
    "# +---data\n",
    "# |   \\---Coronahack-Chest-XRay-Dataset\n",
    "# |       \\---Coronahack-Chest-XRay-Dataset\n",
    "# |           +---SVM\n",
    "# |           |   +---test_modified\n",
    "# |           |   +---test_unique\n",
    "# |           |   +---train_modified\n",
    "# |           |   \\---train_unique\n",
    "# |           +---test\n",
    "# |           \\---train\n",
    "# \\---__pycache__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh SVMsubdirectory structure below\n",
    "# If SVM or any of SVM subdirectory exists, then it removes the SVM subdirectory and create a new SVM folder with the structure below\n",
    "# \n",
    "# +---.ipynb_checkpoints\n",
    "# +---data\n",
    "# |   \\---Coronahack-Chest-XRay-Dataset\n",
    "# |       \\---Coronahack-Chest-XRay-Dataset\n",
    "# |           +---SVM\n",
    "# |           |   +---test_modified\n",
    "# |           |   +---test_unique\n",
    "# |           |   +---train_modified\n",
    "# |           |   \\---train_unique\n",
    "# |           +---test\n",
    "# |           \\---train\n",
    "# \\---__pycache__\n",
    "\n",
    "\n",
    "dir_path = \"data/Coronahack-Chest-XRay-Dataset/Coronahack-Chest-XRay-Dataset\"\n",
    "subpaths = [\"test_modified\",\"train_modified\",\"test_unique\",\"train_unique\"]\n",
    "\n",
    "i = 0\n",
    "loop_count = 0\n",
    "while i < 4:\n",
    "    loop_count += 1\n",
    "    if loop_count == 7:\n",
    "        break\n",
    "    try:\n",
    "        path = dir_path + '/SVM/' + subpaths[i]\n",
    "        os.mkdir(path)\n",
    "    except FileExistsError:\n",
    "        # if folder exist then try remove the whole SVM subdirectory\n",
    "        print(\"Directory \" , 'SVM/' + subpaths[i] ,  \" already exists\")\n",
    "        try:\n",
    "            shutil.rmtree(dir_path + '/SVM/')\n",
    "        except OSError as e:\n",
    "            print(\"Error: %s : %s\" % (dir_path, e.strerror))\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % subpaths[i])\n",
    "        path = dir_path + '/SVM/'\n",
    "        os.mkdir(path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % subpaths[i])\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/Chest_xray_Corona_Metadata.csv\"\n",
    "meta_df = pd.read_csv(filename, header=0)\n",
    "meta_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict mapping image names to labels\n",
    "label_dict = dict()\n",
    "lim = 10\n",
    "i = 0\n",
    "for i, row in meta_df.iterrows():\n",
    "    label_dict[row.X_ray_image_name] = [row.Label, row.Label_2_Virus_category, row.Label_1_Virus_category]\n",
    "CLASS_NAMES = meta_df.Label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_files(path, subpaths,label = False):\n",
    "    \"\"\"\n",
    "    path:\n",
    "    subpath:\n",
    "    labels: boolean\n",
    "    \n",
    "    \"\"\"\n",
    "    f=[]\n",
    "    l=[]\n",
    "    i=0\n",
    "    for subpath in subpaths:\n",
    "        for (dirpath, dirnames, filenames) in walk(\"{}/{}\".format(path,subpath)):\n",
    "            for file in filenames:\n",
    "                f.append(\"{}/{}/{}\".format(path,subpath,file))\n",
    "                if label == True:\n",
    "                    l.append(label_dict[file][0])\n",
    "            break\n",
    "    return f,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_rgb_or_grey(img):\n",
    "    \"\"\"\n",
    "    : Report whether img is RGB or greyscale\n",
    "    \"\"\"\n",
    "    shp = img.shape\n",
    "    color = \"RGB\" if len(shp) == 3 else \"GREY\"\n",
    "    return [shp[0], shp[1], color] \n",
    "\n",
    "def summarize_images(f,meta_df):\n",
    "    '''\n",
    "    : Summarize the images contained in file hierarchy (f)\n",
    "    '''\n",
    "    datasplit_list=[]\n",
    "    row_nums=[]\n",
    "    col_nums=[]\n",
    "    aspect_ratios=[]\n",
    "    name_short=[]\n",
    "    labels_list=[]\n",
    "    file_type=[]\n",
    "    color_list=[]\n",
    "    bad_images=[]\n",
    "    print(\"--- Parsing {} images ---\".format(len(f)))\n",
    "    print_every = 500\n",
    "    i = 0\n",
    "\n",
    "    for filename in f:\n",
    "        file_short = filename.split(\"/\")[-1]\n",
    "        file_ext = filename.split(\".\")[-1]\n",
    "        if ( file_ext in [\"jpeg\",\"jpg\",\"png\"]):\n",
    "            if (i % print_every == 0):\n",
    "                print(\"-> {} images parsed\".format(i))\n",
    "            img = imread(filename)\n",
    "            try:\n",
    "                data_split = meta_df[meta_df.X_ray_image_name == file_short].Dataset_type.values[0]\n",
    "                data_label = meta_df[meta_df.X_ray_image_name == file_short].Label.values[0]\n",
    "                datasplit_list.append(data_split)\n",
    "                rows, cols, color = shape_rgb_or_grey(img)\n",
    "                row_nums.append(rows)\n",
    "                col_nums.append(cols)\n",
    "                aspect_ratios.append(cols/rows)\n",
    "                name_short.append(filename.split(\"/\")[-1])\n",
    "                labels_list.append(data_label)\n",
    "                file_type.append(file_ext)\n",
    "                color_list.append(color)\n",
    "                i += 1\n",
    "            except ValueError:\n",
    "                print(ValueError)\n",
    "                print(\"{} - shape = {}\".format(filename, img.shape))\n",
    "            except IndexError:\n",
    "                bad_images.append(file_short)\n",
    "                print(f'--> {file_short} is not in metadata. Skipping.')\n",
    "            except:\n",
    "                print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "                print(file_short)\n",
    "                raise\n",
    "\n",
    "        else:\n",
    "            print(\"Skipping {}.\".format(filename))\n",
    "\n",
    "    # summarize image sizes\n",
    "    data = {\"name\": name_short, \"dataset_type\": datasplit_list, \"ext\": file_type, \"color\": color_list, \"label\": labels_list, \"n_rows\": row_nums, \"n_cols\": col_nums, \"asp_ratio\": aspect_ratios}\n",
    "    df_sizes = pd.DataFrame(data=data,columns=[\"name\", \"dataset_type\", \"ext\", \"color\", \"label\", \"n_rows\", \"n_cols\", \"asp_ratio\"])\n",
    "    print(f\"--- There were {len(bad_images)} images not included in the metadata .csv ---\")\n",
    "    return [df_sizes, bad_images]\n",
    "\n",
    "\n",
    "# visualize distribution of image sizes\n",
    "def df_size_plots(df_sizes):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(10,3))\n",
    "    # img size distribution\n",
    "    hist2d = axs[0].hist2d(df_sizes[\"n_rows\"],df_sizes[\"n_cols\"],bins=50, cmap=plt.cm.jet)\n",
    "    plt.colorbar(hist2d[3],ax=axs[0])\n",
    "    axs[0].set_title(\"Image Size Distribution\")\n",
    "    axs[0].set_xlabel(\"# Rows\")\n",
    "    axs[0].set_ylabel(\"# Cols\")\n",
    "    # rgb/color distribution\n",
    "    colors = [\"RGB\",\"GREY\"]\n",
    "    vals = [df_sizes[df_sizes[\"color\"]==color].shape[0] for color in colors]\n",
    "    axs[1].bar(colors,vals)\n",
    "    axs[1].set_title(\"Image Color Distribution\")\n",
    "    axs[1].set_ylabel(\"Count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path=\"data/Coronahack-Chest-XRay-Dataset/Coronahack-Chest-XRay-Dataset\"\n",
    "subpaths=[\"test\",\"train\"]\n",
    "f,f_label = collect_files(path, subpaths)\n",
    "df_sizes, bad_images = summarize_images(f, meta_df)\n",
    "df_sizes.describe()\n",
    "df_size_plots(df_sizes)\n",
    "row_mean = df_sizes.n_rows.mean()\n",
    "col_mean = df_sizes.n_cols.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RESIZE_BOOL = True\n",
    "df_sizes.n_rows.mean()\n",
    "ROWS = int(row_mean/4)\n",
    "COLS = int(col_mean/4)\n",
    "           \n",
    "\n",
    "path=\"data/Coronahack-Chest-XRay-Dataset/Coronahack-Chest-XRay-Dataset\"\n",
    "source_subpaths=[\"test\",\"train\"]\n",
    "dest_subpaths = [\"SVM/test_modified\",\"SVM/train_modified\"]\n",
    "\n",
    "# source_path=\"data/Coronahack-Chest-XRay-Dataset/Coronahack-Chest-XRay-Dataset/test\"\n",
    "# dest_subpath = \"data/Coronahack-Chest-XRay-Dataset/Coronahack-Chest-XRay-Dataset/test_modified\"\n",
    "# convert_image(source_path,dest_subpath,COLS,ROWS,RESIZE_BOOL,False,False,10,False,50,1,False,0.2,False,0.2,True,600,True,595,label_dict,print_every=200)\n",
    "   \n",
    "\n",
    "for i in range(len(source_subpaths)):\n",
    "    test_path = path + '/' +  source_subpaths[i]\n",
    "    test_destination_path =  path + '/' +  dest_subpaths[i]\n",
    "    convert_image(test_path,test_destination_path,COLS,ROWS,RESIZE_BOOL,False,False,10,False,50,1,False,0.2,False,0.2,True,max(COLS,ROWS)+1,True,595,label_dict,print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"data/Coronahack-Chest-XRay-Dataset/Coronahack-Chest-XRay-Dataset\"\n",
    "subpaths=[\"SVM/test_modified\",\"SVM/train_modified\"]\n",
    "f,f_label = collect_files(path, subpaths)\n",
    "df_sizes, bad_images = summarize_images(f, meta_df)\n",
    "df_sizes.describe()\n",
    "df_size_plots(df_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"data/Coronahack-Chest-XRay-Dataset/Coronahack-Chest-XRay-Dataset\"\n",
    "source_subpaths=[\"SVM/test_modified\",\"SVM/train_modified\"]\n",
    "dest_subpaths = [\"SVM/test_unique\",\"SVM/train_unique\"]\n",
    "\n",
    "for i in range(len(subpaths)):\n",
    "    test_path = path + '/' +  source_subpaths[i]\n",
    "    test_destination_path =  path + '/' +  dest_subpaths[i]\n",
    "    list_unique_x_ray(test_path,test_destination_path,False,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"data/Coronahack-Chest-XRay-Dataset/Coronahack-Chest-XRay-Dataset\"\n",
    "subpaths = [\"SVM/test_unique\",\"SVM/train_unique\"]\n",
    "\n",
    "f,f_label = collect_files(path, subpaths, True)\n",
    "df_sizes, bad_images = summarize_images(f, meta_df)\n",
    "df_sizes.describe()\n",
    "df_size_plots(df_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hog_descriptor(f_paths,image = False): \n",
    "    \"\"\"\n",
    "    f_path: path of crawled images\n",
    "    image: set to True to capture hod image array for ALL images,\n",
    "           set to a list to capature only the indexes in the list\n",
    "    \"\"\"\n",
    "    hog_fd = []\n",
    "    img_dict = {}\n",
    "    i = 0\n",
    "    print_every = 500\n",
    "        \n",
    "    for filename in f_paths:\n",
    "        file_ext = filename.split(\".\")[-1]\n",
    "        if ( file_ext in [\"jpeg\",\"jpg\",\"png\"]):\n",
    "            if (i % print_every == 0):\n",
    "                print(\"-> {} images parsed\".format(i))\n",
    "    \n",
    "            img = imread(filename)\n",
    "            try:\n",
    "                if ((image == True) or (i in image)):\n",
    "                    fd, hog_image = hog(img, orientations=8, pixels_per_cell=(16, 16), cells_per_block=(2, 2),  transform_sqrt=True, block_norm=\"L1\", visualize=True)\n",
    "                    hog_fd.append(fd)\n",
    "                    img_dict[i]= hog_image                \n",
    "                else:\n",
    "                    fd = hog(img, orientations=8, pixels_per_cell=(16, 16), cells_per_block=(2, 2),  transform_sqrt= True, block_norm=\"L1\")\n",
    "                    hog_fd.append(fd)\n",
    "\n",
    "                \n",
    "                i += 1\n",
    "            except ValueError:\n",
    "                print(ValueError)\n",
    "                print(\"{} - shape = {}\".format(filename, img.shape))\n",
    "            except:\n",
    "                print(\"Unknown exception occurred.\") \n",
    "            \n",
    "        else:\n",
    "            print(\"Skipping {}.\".format(filename))\n",
    "            \n",
    "    return(hog_fd, img_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subpaths = [\"SVM/train_unique\"]\n",
    "\n",
    "f_train,label_train = collect_files(path, subpaths, True)\n",
    "\n",
    "#create hog decriptions for modified training set\n",
    "hog_train, img_train = hog_descriptor(f_train,[1,2,3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hog_train), len(f_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_img_train = imread(f_train[2])\n",
    "fig, ax= plt.subplots(1,2,figsize=(10,5))\n",
    "ax[0].imshow(img_train[2]/img_train[2].max()) \n",
    "ax[1].imshow(ex_img_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subpaths = [\"SVM/test_unique\"]\n",
    "\n",
    "f_test, label_test = collect_files(path, subpaths, True)\n",
    "\n",
    "#create hog decriptions for modified test set\n",
    "hog_test, img_test = hog_descriptor(f_test, [2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hog_test)+len(hog_train))\n",
    "print(len(hog_test)+len(hog_train) == len(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_img_test = imread(f_test[2])\n",
    "fig, ax= plt.subplots(1,2,figsize=(10,5))\n",
    "ax[0].imshow(img_test[2]/img_test[2].max()) \n",
    "ax[1].imshow(ex_img_test)\n",
    "# imshow(img_test[2]/img_test[2].max()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_stack(hog_fd,label):\n",
    "    label =  np.array(label).reshape(len(label),1)\n",
    "    hog_fd = np.array(hog_fd)\n",
    "    \n",
    "    data_array = np.hstack((hog_fd,label))\n",
    "    \n",
    "    x = data_array[:,:-1]\n",
    "    y = data_array[:,-1:].ravel()\n",
    "    \n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train = data_stack(hog_train,label_train)\n",
    "x_test,y_test = data_stack(hog_test,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {'C': [0.1,1, 10], 'gamma': [1,0.1,0.01],'kernel': ['linear','rbf', 'poly']}\n",
    "# model = GridSearchCV(SVC(),param_grid,refit=True,verbose=2, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(StandardScaler(), SVC(C=1, gamma=0.1, kernel='poly',probability = True,random_state = 7))\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_probs = model.predict_proba(x_test)\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_dur, tpr_dur,_ = metrics.roc_curve(y_test,predictions_probs[:,1] ,pos_label='Pnemonia')\n",
    "auc = metrics.roc_auc_score(y_test,predictions_probs[:,1])\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(fpr_dur,tpr_dur,label='(auc={:3.2f})'.format(auc))\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_nn, rec_nn, threshs = metrics.precision_recall_curve(y_test,predictions_probs[:,1],pos_label='Pnemonia')\n",
    "auc = metrics.auc(rec_nn, prec_nn)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(rec_nn,prec_nn,label='(auc={:3.2f})'.format(auc))\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('precision')\n",
    "plt.title(\"PR curve\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"y_test\": y_test, \"y_hat\":predictions}\n",
    "df = pd.DataFrame(data, columns=[\"y_test\", \"y_hat\"])\n",
    "confusion_matrix = pd.crosstab(df['y_test'],df['y_hat'],rownames=[\"Actual\"],colnames=['Predicted'])\n",
    "f, ax = plt.subplots(figsize=(7,5))\n",
    "sn.heatmap(confusion_matrix, annot=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
